{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e721296",
   "metadata": {},
   "source": [
    "## Web-Scraping Script for Fotmob\n",
    "*Author: James Njoroge*\n",
    "\n",
    "Scraping to collect the following metrics from the fotmob website for the 2024/2025 Premier League Season:\n",
    "### Match Data:\n",
    "1. **Label (match-wise, post-match)**:\n",
    "    - Goals\n",
    "    - Fouls\n",
    "    - Expected Goals (xG)\n",
    "    - Big chances\n",
    "    - Total Shots\n",
    "    - Shots on Target\n",
    "    - Shots Inside Box\n",
    "    - Corners\n",
    "    - Touches in opposition box\n",
    "    - Stadium attendance vs. Stadium Capacity (*Factor in the fan demand since they play a hand in influencing player behavior.*)\n",
    "2. **Features (team-wise, pre-match)**:\n",
    "    - xG/90\n",
    "    - shots on target/90\n",
    "    - corners/90\n",
    "    - xG conceded/90\n",
    "    - Home advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb377f",
   "metadata": {},
   "source": [
    "### Intuition:\n",
    "\n",
    "1. From a bit of observation using the browser developer tools, we realise that whenever we look at a specific match's page or statistics, Fotmob will query (via api) the data from the backend and that is returned by a json, that is then parsed to pass the statistics on to the UI. (See this from inspecting the network tab)\n",
    "2. What we need is that json file and in our case, we need to do it for all 380 matches that took place in the 24-25 season. \n",
    "3. Therefore, the next logical question is whether we can intercept the data json file as it is being passed to the frontend. We can using selenium-wire. The flow is like this: \n",
    "    - We go to the fotmob page with all the rounds matches (38 rounds).\n",
    "    - From this page we can pull the links for each specific match from that round by targeting the div classes and the anchor classes.\n",
    "    - Then from here we visit all of these match links and intercept the data json file that has been requested from the backend using there private API.\n",
    "    - In a few cases, you'll find that there was no data file saved, because the page probably took too long to load, this is okay and as long as it's one or two files, you can go to the browser yourself and pull that json file from the network tab and add it to your directory. \n",
    "4. This json file has all of the data and metadata needed for each match (including the prevailing weather conditions) so we simply save it and can process the parts that we need later on.\n",
    "5. Having tried querying the fotmob API directly through the browser, we see that it is pretty much locked (good programming practice) so we simply take the long route. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fc1f6",
   "metadata": {},
   "source": [
    "### Already scraped data for reference\n",
    "\n",
    "I have provided my scraped match json files in the ```24-25_PL_Data_raw.zip``` file for your use or reference if you decide to test out the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d0957",
   "metadata": {},
   "source": [
    "### Setup: Load Libraries and Season Configuration\n",
    "\n",
    "1. Align all scraping dependencies (Selenium Wire, requests, compression helpers) so the notebook can monitor network traffic reliably.\n",
    "2. Lock in the league/season identifiers plus filesystem and Selenium knobs that the rest of the flow reuses.\n",
    "3. Next we package small utilities around these constants to keep the later scraping loop tidy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d45ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for coding and structures\n",
    "import os, re, json, time, random, gzip, zlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import brotli        # for Content-Encoding: br\n",
    "import requests      # optional direct fallback\n",
    "\n",
    "# --- Selenium (network interception via selenium-wire)\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "SEASON_LABEL       = \"24-25_PL_Data\"       # top-level folder\n",
    "LEAGUE_ID          = 47                    # Premier League (can check fotmob for other league codes)\n",
    "LEAGUE_SLUG        = \"premier-league\"      # basically how fotmob denotes their league name\n",
    "SEASON_SLUG        = \"2024-2025\"           # specific season in fotmob's format\n",
    "ROUND_START        = 0                     # you observed 0..37\n",
    "ROUND_END          = 37\n",
    "DATA_ROOT          = SEASON_LABEL\n",
    "CHROMEDRIVER_PATH  = os.path.expanduser(\"/Path/to/your/chromedriver-mac-arm64/chromedriver\") # Using mac here but change it to your systems path (plus download your chromedriver from chrome developer tools website)\n",
    "HEADLESS           = True                  # do the task without visible user interface (in the background)\n",
    "PER_MATCH_SLEEP    = 0.25\n",
    "PAGE_SETTLE_WAIT   = 3.0                   # give a bit more time for all elements and queries to load\n",
    "RETRY_ON_MISS      = 1                     # extra quick retry if miss\n",
    "ALLOW_DIRECT_FALLBACK = True               # try direct GET if interception misses\n",
    "DEBUG              = False                 # set True to emit extra logs + snapshots\n",
    "\n",
    "# exact classes where the matches are listed on a frontend website\n",
    "SECTION_CLASS_EXACT = \"css-o4yr0b-LeagueMatchesSectionCSS eaa01ac0\"\n",
    "ANCHOR_CLASS_EXACT  = \"css-1ajdexg-MatchWrapper e1mxmq6p0\"\n",
    "\n",
    "ROUND_URL_TMPL = (\n",
    "    \"https://www.fotmob.com/leagues/{league_id}/matches/{league_slug}\"\n",
    "    \"?season={season}&group=by-round&round={round_no}\"\n",
    ") # url template for the matchpages for each round (note league_id, season, and round numbers)\n",
    "\n",
    "# Accept BOTH endpoints (some matches use /api/matchDetails, others /api/data/matchDetails)\n",
    "DETAILS_PATTERNS = [\n",
    "    re.compile(r\"https://(?:www\\.)?fotmob\\.com/api/data/matchDetails\\?matchId=(\\d+)\"),\n",
    "    re.compile(r\"https://(?:www\\.)?fotmob\\.com/api/matchDetails\\?matchId=(\\d+)\")\n",
    "]\n",
    "# accept /matches/...#<id>  OR  /match/<id>\n",
    "MATCH_HREF_RX = re.compile(r\"(?:/match(?:es)?/[^#]*#|/match/)(\\d+)\")\n",
    "\n",
    "# Ensure that filepaths to save the data files exist and different data path for debugging, just in case.\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "DEBUG_DIR = os.path.join(DATA_ROOT, \"_debug\")\n",
    "if DEBUG: os.makedirs(DEBUG_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587b65d",
   "metadata": {},
   "source": [
    "### Utilities: Shared Helpers\n",
    "\n",
    "1. Bundle Selenium bootstrap, directory creation, and ID parsing helpers so later cells stay focused on workflow logic.\n",
    "2. Centralise response decoding and safe file naming, giving us reusable building blocks for every match we touch.\n",
    "3. With these basics covered, we can concentrate on discovering the per-round match URLs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c534428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "# Bootstrap selenium-wire so we can capture network traffic from match pages.\n",
    "def init_driver(headless: bool = True):\n",
    "    \"\"\"Initialise a selenium-wire Chrome driver using the configured Chrome options.\"\"\"\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=opts)  # selenium-wire driver\n",
    "    driver.set_page_load_timeout(45)\n",
    "    return driver\n",
    "\n",
    "# Keep a per-round folder structure for the downloaded JSON files.\n",
    "def ensure_round_dir(round_no: int) -> str:\n",
    "    \"\"\"Create (if needed) and return the filesystem path for a given round.\"\"\"\n",
    "    rdir = os.path.join(DATA_ROOT, f\"round_{round_no}\")\n",
    "    os.makedirs(rdir, exist_ok=True)\n",
    "    return rdir\n",
    "\n",
    "# Pull out the matchId token from Fotmob anchor links.\n",
    "def match_id_from_href(href: str) -> Optional[str]:\n",
    "    \"\"\"Extract the matchId token from a Fotmob anchor href.\"\"\"\n",
    "    if not href:\n",
    "        return None\n",
    "    m = MATCH_HREF_RX.search(href)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# Case-insensitive header lookup helper for captured responses.\n",
    "def get_header(headers, key: str) -> Optional[str]:\n",
    "    \"\"\"Retrieve a header value in a case-insensitive way from selenium-wire metadata.\"\"\"\n",
    "    if not headers:\n",
    "        return None\n",
    "    key = key.lower()\n",
    "    for k, v in headers.items():\n",
    "        if k.lower() == key:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# Normalise compressed response bodies to readable JSON bytes.\n",
    "def decode_body(raw: bytes, headers: Dict) -> bytes:\n",
    "    \"\"\"Decompress the response body based on Content-Encoding and return raw JSON bytes.\"\"\"\n",
    "    enc = (get_header(headers, \"Content-Encoding\") or \"\").lower()\n",
    "    if \"br\" in enc:\n",
    "        return brotli.decompress(raw)\n",
    "    if \"gzip\" in enc:\n",
    "        return gzip.decompress(raw)\n",
    "    if \"deflate\" in enc:\n",
    "        # some servers use raw deflate (zlib) â€” try both\n",
    "        try:\n",
    "            return zlib.decompress(raw)\n",
    "        except zlib.error:\n",
    "            return zlib.decompress(raw, -zlib.MAX_WBITS)\n",
    "    return raw  # no compression\n",
    "\n",
    "# Persist intercepted payloads in a pretty-printed format for inspection.\n",
    "def save_json_pretty(path: str, raw_json_bytes: bytes):\n",
    "    \"\"\"Write the captured payload to disk with readable indentation.\"\"\"\n",
    "    data = json.loads(raw_json_bytes.decode(\"utf-8\"))\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Sanitise strings when embedding team names into filenames.\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Sanitise strings for filesystem-friendly filenames.\"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9_\\-]+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "# Lightweight debug logger toggled via the DEBUG flag.\n",
    "def _dbg(msg: str):\n",
    "    \"\"\"Emit debug output when verbose tracing is enabled.\"\"\"\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "# Optional HTML snapshots make debugging missing elements easier.\n",
    "def _save_snapshot(driver, round_no, tag):\n",
    "    \"\"\"Persist the HTML snapshot for troubleshooting when DEBUG mode is active.\"\"\"\n",
    "    if not DEBUG:\n",
    "        return\n",
    "    p = os.path.join(DEBUG_DIR, f\"round_{round_no}_{tag}.html\")\n",
    "    try:\n",
    "        with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(f\"[SNAPSHOT] {p}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Limit selenium-wire capture scope to matchDetails requests only.\n",
    "def details_scope_pattern() -> str:\n",
    "    \"\"\"Limit selenium-wire interception to matchDetails endpoints across both URL patterns.\"\"\"\n",
    "    return r\".*fotmob\\.com/api/(?:data/)?matchDetails\\?matchId=\\d+.*\"\n",
    "\n",
    "# Extract matchId values from captured request URLs.\n",
    "def extract_mid_from_url(url: str) -> Optional[str]:\n",
    "    \"\"\"Pull the matchId from a captured request URL using the configured regex patterns.\"\"\"\n",
    "    for rx in DETAILS_PATTERNS:\n",
    "        m = rx.search(url)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000e547",
   "metadata": {},
   "source": [
    "### Discovery: Collect Match Links Per Round\n",
    "\n",
    "1. Visit the Fotmob round page and target the match containers using the observed CSS classes.\n",
    "2. Fall back to a broader anchor scan when the exact selectors miss, deduplicating links as we go.\n",
    "3. Once we have the URLs, we can intercept the matchDetails payloads for each fixture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd5d2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Discovery (exact classes first, then fallback)\n",
    "# =========================\n",
    "def discover_match_urls_for_round(round_no: int) -> List[str]:\n",
    "    url = ROUND_URL_TMPL.format(\n",
    "        league_id=LEAGUE_ID,\n",
    "        league_slug=LEAGUE_SLUG,\n",
    "        season=SEASON_SLUG,\n",
    "        round_no=round_no,\n",
    "    )\n",
    "    driver = init_driver(headless=HEADLESS)\n",
    "    try:\n",
    "        _dbg(f\"[URL] navigating -> {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1.0)\n",
    "        _dbg(f\"[URL] landed     -> {driver.current_url}\")\n",
    "\n",
    "        # Wait for sections with exact class\n",
    "        section_css_exact = \"section.\" + \".\".join(SECTION_CLASS_EXACT.split())\n",
    "        sections = []\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, section_css_exact))\n",
    "            )\n",
    "            sections = driver.find_elements(By.CSS_SELECTOR, section_css_exact)\n",
    "            _dbg(f\"[SECTIONS:EXACT] {len(sections)} via '{section_css_exact}'\")\n",
    "        except TimeoutException:\n",
    "            _dbg(f\"[SECTIONS:EXACT] none after wait\")\n",
    "            _save_snapshot(driver, round_no, \"no_sections_exact\")\n",
    "\n",
    "        links: List[str] = []\n",
    "\n",
    "        if sections:\n",
    "            anchor_css_exact = \"a.\" + \".\".join(ANCHOR_CLASS_EXACT.split())\n",
    "            for sec in sections:\n",
    "                anchors = sec.find_elements(By.CSS_SELECTOR, f'{anchor_css_exact}[href]')\n",
    "                for a in anchors:\n",
    "                    href = a.get_attribute(\"href\")\n",
    "                    if not href: continue\n",
    "                    href = urljoin(\"https://www.fotmob.com\", href)\n",
    "                    if \"/matches/\" in href or \"/match/\" in href:\n",
    "                        links.append(href)\n",
    "\n",
    "        # Fallback: whole page scan\n",
    "        if not links:\n",
    "            time.sleep(1.0)\n",
    "            anchors = driver.find_elements(By.CSS_SELECTOR, 'a[href^=\"/matches/\"], a[href*=\"/match/\"]')\n",
    "            for a in anchors:\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if not href: continue\n",
    "                href = urljoin(\"https://www.fotmob.com\", href)\n",
    "                if \"/matches/\" in href or \"/match/\" in href:\n",
    "                    links.append(href)\n",
    "\n",
    "        # Deduplicate preserving order\n",
    "        seen, out = set(), []\n",
    "        for h in links:\n",
    "            if h not in seen:\n",
    "                seen.add(h)\n",
    "                out.append(h)\n",
    "        return out\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d9c27",
   "metadata": {},
   "source": [
    "### Interception: Capture `matchDetails` Payloads\n",
    "\n",
    "1. Spin up a fresh Selenium Wire session for each match URL so the request log starts clean and we do not carry over stale interceptions between fixtures.\n",
    "2. Decode any compressed responses and retry or fall back to direct API calls when interception misses.\n",
    "3. The returned bytes will feed the lightweight parsers that pull team names and support on-disk storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bc9278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Interception (selenium-wire) with decode + retries + fallback\n",
    "# =========================\n",
    "def intercept_matchdetails_for_url(match_url: str, expected_match_id: Optional[str]) -> Optional[Tuple[str, bytes]]:\n",
    "    \"\"\"\n",
    "    Opens the match page, captures network responses, and returns\n",
    "    (api_url, decoded_json_bytes) for the *current* matchId.\n",
    "    \"\"\"\n",
    "    opts = Options()\n",
    "    if HEADLESS:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    service = Service(CHROMEDRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=opts)  # selenium-wire driver\n",
    "    try:\n",
    "        driver.scopes = [details_scope_pattern()]\n",
    "        driver.requests.clear()\n",
    "\n",
    "        driver.get(match_url)\n",
    "        time.sleep(PAGE_SETTLE_WAIT)\n",
    "\n",
    "        # small nudge; some pages lazy-load\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollBy(0, 300);\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        def pick_matchdetails() -> Optional[Tuple[str, bytes]]:\n",
    "            hits = []\n",
    "            for req in driver.requests:\n",
    "                if not req.response: \n",
    "                    continue\n",
    "                mid_in_url = extract_mid_from_url(req.url)\n",
    "                if not mid_in_url:\n",
    "                    continue\n",
    "                if expected_match_id and mid_in_url != expected_match_id:\n",
    "                    # skip head-to-head calls for other matches\n",
    "                    continue\n",
    "                try:\n",
    "                    raw = req.response.body or b\"\"\n",
    "                except Exception:\n",
    "                    raw = b\"\"\n",
    "                if not raw:\n",
    "                    continue\n",
    "                # decompress to readable JSON\n",
    "                decoded = decode_body(raw, req.response.headers or {})\n",
    "                # quick sanity: ensure it parses\n",
    "                try:\n",
    "                    json.loads(decoded.decode(\"utf-8\"))\n",
    "                except Exception:\n",
    "                    continue\n",
    "                hits.append((req.url, decoded))\n",
    "            # return the first good hit (usually only one)\n",
    "            return hits[0] if hits else None\n",
    "\n",
    "        hit = pick_matchdetails()\n",
    "\n",
    "        # quick retry if miss\n",
    "        attempts = 0\n",
    "        while not hit and attempts < RETRY_ON_MISS:\n",
    "            time.sleep(1.5)\n",
    "            try:\n",
    "                driver.execute_script(\"window.scrollBy(0, 600);\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            hit = pick_matchdetails()\n",
    "            attempts += 1\n",
    "\n",
    "        # OPTIONAL: direct fallback (single polite GET) if still missing\n",
    "        if not hit and ALLOW_DIRECT_FALLBACK and expected_match_id:\n",
    "            try:\n",
    "                # prefer /api/data/matchDetails; fallback to /api/matchDetails\n",
    "                sess = requests.Session()\n",
    "                sess.headers.update({\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "                                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127 Safari/537.36\",\n",
    "                    \"Accept\": \"application/json, text/plain, */*\",\n",
    "                    \"Referer\": \"https://www.fotmob.com/\",\n",
    "                })\n",
    "                urls = [\n",
    "                    f\"https://www.fotmob.com/api/data/matchDetails?matchId={expected_match_id}\",\n",
    "                    f\"https://www.fotmob.com/api/matchDetails?matchId={expected_match_id}\",\n",
    "                ]\n",
    "                for u in urls:\n",
    "                    r = sess.get(u, timeout=20)\n",
    "                    if r.ok and r.content:\n",
    "                        # requests auto-decompresses; just sanity-parse\n",
    "                        json.loads(r.content.decode(\"utf-8\"))\n",
    "                        return (u, r.content)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return hit\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeebabee",
   "metadata": {},
   "source": [
    "### Parsing: Pull Team Metadata from Payloads\n",
    "\n",
    "1. Quickly examine the JSON structure for the header/general sections to extract the home and away names.\n",
    "2. Provide graceful fallbacks when the structure varies so downstream code can still log the match.\n",
    "3. With this helper, saved files carry human-readable context in their filenames and index entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad9aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Parse minimal team info\n",
    "# =========================\n",
    "def extract_teams_from_payload(payload: bytes) -> Tuple[Optional[str], Optional[str]]:\n",
    "    try:\n",
    "        d = json.loads(payload.decode(\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception:\n",
    "        return (None, None)\n",
    "\n",
    "    header = d.get(\"header\") if isinstance(d, dict) else None\n",
    "    if isinstance(header, dict):\n",
    "        t = header.get(\"teams\")\n",
    "        if isinstance(t, list) and len(t) >= 2:\n",
    "            home = t[0].get(\"name\") if isinstance(t[0], dict) else None\n",
    "            away = t[1].get(\"name\") if isinstance(t[1], dict) else None\n",
    "            if home or away:\n",
    "                return (home, away)\n",
    "        home = header.get(\"home\", {}).get(\"name\")\n",
    "        away = header.get(\"away\", {}).get(\"name\")\n",
    "        if home or away:\n",
    "            return (home, away)\n",
    "\n",
    "    general = d.get(\"general\") if isinstance(d, dict) else None\n",
    "    if isinstance(general, dict):\n",
    "        h = general.get(\"homeTeam\", {})\n",
    "        a = general.get(\"awayTeam\", {})\n",
    "        home = h.get(\"name\") if isinstance(h, dict) else None\n",
    "        away = a.get(\"name\") if isinstance(a, dict) else None\n",
    "        if home or away:\n",
    "            return (home, away)\n",
    "\n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c032c",
   "metadata": {},
   "source": [
    "### Orchestration: Scrape the Entire Season\n",
    "\n",
    "1. Loop across every round, use the discovery helpers to gather fixtures, and intercept the corresponding matchDetails JSON.\n",
    "2. Persist each payload with clear filenames, build an index of metadata, and respect delays to keep the crawl polite.\n",
    "3. After the loop, write an index.json summary that fuels validation and downstream analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0376a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[URL] navigating -> https://www.fotmob.com/leagues/47/matches/premier-league?season=2024-2025&group=by-round&round=0\n",
      "[URL] landed     -> https://www.fotmob.com/leagues/47/matches/premier-league?season=2024-2025&group=by-round&round=0\n",
      "[SECTIONS:EXACT] 4 via 'section.css-o4yr0b-LeagueMatchesSectionCSS.eaa01ac0'\n",
      "Round 0: 10 matches\n",
      "  [1/10] saved 4506263 -> 24-25_PL_Data/round_0/4506263_matchDetails_Manchester_United-vs-Fulham.json\n",
      "  [2/10] saved 4506264 -> 24-25_PL_Data/round_0/4506264_matchDetails_Ipswich_Town-vs-Liverpool.json\n",
      "  [3/10] saved 4506265 -> 24-25_PL_Data/round_0/4506265_matchDetails_Arsenal-vs-Wolverhampton_Wanderers.json\n",
      "  [4/10] saved 4506266 -> 24-25_PL_Data/round_0/4506266_matchDetails_Everton-vs-Brighton_Hove_Albion.json\n",
      "  [5/10] saved 4506267 -> 24-25_PL_Data/round_0/4506267_matchDetails_Newcastle_United-vs-Southampton.json\n",
      "  [6/10] saved 4506268 -> 24-25_PL_Data/round_0/4506268_matchDetails_Nottingham_Forest-vs-AFC_Bournemouth.json\n",
      "  [7/10] saved 4506269 -> 24-25_PL_Data/round_0/4506269_matchDetails_West_Ham_United-vs-Aston_Villa.json\n",
      "  [8/10] saved 4506270 -> 24-25_PL_Data/round_0/4506270_matchDetails_Brentford-vs-Crystal_Palace.json\n",
      "  [9/10] saved 4506271 -> 24-25_PL_Data/round_0/4506271_matchDetails_Chelsea-vs-Manchester_City.json\n",
      "  [10/10] saved 4506272 -> 24-25_PL_Data/round_0/4506272_matchDetails_Leicester_City-vs-Tottenham_Hotspur.json\n",
      "\n",
      "Wrote index -> 24-25_PL_Data/index.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    index_rows: List[Dict] = []\n",
    "\n",
    "    for round_no in range(ROUND_START, ROUND_END + 1):\n",
    "        round_dir = ensure_round_dir(round_no)\n",
    "\n",
    "        urls = discover_match_urls_for_round(round_no)\n",
    "\n",
    "        # keep only links that yield a parseable matchId\n",
    "        cleaned = []\n",
    "        seen = set()\n",
    "        for u in urls:\n",
    "            mid = match_id_from_href(u)\n",
    "            if not mid:\n",
    "                continue\n",
    "            if u not in seen:\n",
    "                seen.add(u)\n",
    "                cleaned.append(u)\n",
    "\n",
    "        print(f\"Round {round_no}: {len(cleaned)} matches\")\n",
    "\n",
    "        for i, u in enumerate(cleaned, 1):\n",
    "            mid = match_id_from_href(u)\n",
    "            saved_path = None\n",
    "            api_url = None\n",
    "            home = away = None\n",
    "\n",
    "            hit = intercept_matchdetails_for_url(u, mid)\n",
    "            if hit:\n",
    "                api_url, payload = hit\n",
    "                home, away = extract_teams_from_payload(payload)\n",
    "\n",
    "                base = f\"{mid}_matchDetails\"\n",
    "                if home and away:\n",
    "                    base += f\"_{safe_name(home)}-vs-{safe_name(away)}\"\n",
    "                fname = base + \".json\"\n",
    "                fpath = os.path.join(round_dir, fname)\n",
    "                save_json_pretty(fpath, payload)          # <-- pretty, readable JSON\n",
    "                saved_path = fpath\n",
    "                print(f\"  [{i}/{len(cleaned)}] saved {mid} -> {os.path.relpath(fpath)}\")\n",
    "            else:\n",
    "                print(f\"  [{i}/{len(cleaned)}] missed {mid} (no matchDetails)\")\n",
    "                # (It will still be recorded in index)\n",
    "\n",
    "            index_rows.append({\n",
    "                \"round\": round_no,\n",
    "                \"matchUrl\": u,\n",
    "                \"matchId\": mid,\n",
    "                \"apiUrl\": api_url,\n",
    "                \"home\": home,\n",
    "                \"away\": away,\n",
    "                \"jsonPath\": os.path.relpath(saved_path) if saved_path else None,\n",
    "                \"ts\": time.time(),\n",
    "            })\n",
    "\n",
    "            time.sleep(PER_MATCH_SLEEP + random.uniform(0, 0.15))\n",
    "\n",
    "    # write season-level index\n",
    "    index_path = os.path.join(DATA_ROOT, \"index.json\")\n",
    "    with open(index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(index_rows, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nWrote index -> {index_path}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6272631",
   "metadata": {},
   "source": [
    "### QA: Verify Download Coverage\n",
    "\n",
    "1. After scraping, sweep the filesystem to count distinct matchIds and confirm the expected range is present.\n",
    "2. Surface any missing or unexpected IDs so we know instantly which fixtures need another pass.\n",
    "3. This quick audit runs standalone, letting us rerun it after manual file changes or re-scrapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ab783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 380 match JSON files (expected 380).\n",
      "Observed matchId range: 4506263 - 4506642\n",
      "Missing matchIds: none\n",
      "Unexpected matchIds: none\n"
     ]
    }
   ],
   "source": [
    "# Post-scraping validation of downloaded match JSON files\n",
    "from pathlib import Path\n",
    "\n",
    "EXPECTED_MATCH_COUNT = 380\n",
    "EXPECTED_MATCH_ID_START = 4506263 # Pull from fotmob website\n",
    "EXPECTED_MATCH_ID_END = 4506642 # Pull from fotmob website / should be the starting match id plus 379 due to zero-based indexing.\n",
    "\n",
    "\n",
    "def summarize_match_downloads(data_root: str = DATA_ROOT) -> None:\n",
    "    data_root_path = Path(data_root)\n",
    "    if not data_root_path.exists():\n",
    "        print(f\"Data root {data_root} does not exist.\")\n",
    "        return\n",
    "\n",
    "    match_ids = []\n",
    "    for round_dir in sorted(data_root_path.glob('round_*')):\n",
    "        for json_path in round_dir.glob('*.json'):\n",
    "            stem = json_path.stem\n",
    "            match_id_part = stem.split('_', 1)[0]\n",
    "            if match_id_part.isdigit():\n",
    "                match_ids.append(int(match_id_part))\n",
    "\n",
    "    unique_ids = sorted(set(match_ids))\n",
    "    expected_ids = set(range(EXPECTED_MATCH_ID_START, EXPECTED_MATCH_ID_END + 1))\n",
    "    missing_ids = sorted(expected_ids - set(unique_ids))\n",
    "    unexpected_ids = sorted(set(unique_ids) - expected_ids)\n",
    "\n",
    "    print(f\"Found {len(unique_ids)} match JSON files (expected {EXPECTED_MATCH_COUNT}).\")\n",
    "    if unique_ids:\n",
    "        print(f\"Observed matchId range: {unique_ids[0]} - {unique_ids[-1]}\")\n",
    "    else:\n",
    "        print('Observed matchId range: none')\n",
    "\n",
    "    if missing_ids:\n",
    "        print(f\"Missing matchIds ({len(missing_ids)}): {', '.join(str(mid) for mid in missing_ids)}\")\n",
    "    else:\n",
    "        print('Missing matchIds: none')\n",
    "\n",
    "    if unexpected_ids:\n",
    "        print(f\"Unexpected matchIds ({len(unexpected_ids)}): {', '.join(str(mid) for mid in unexpected_ids)}\")\n",
    "    else:\n",
    "        print('Unexpected matchIds: none')\n",
    "\n",
    "\n",
    "summarize_match_downloads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654adad",
   "metadata": {},
   "source": [
    "### Recovery: Reconstruct `index.json` from Disk\n",
    "\n",
    "1. When the on-disk index drifts, rebuild it by reading each round's JSON files and extracting match metadata.\n",
    "2. Recreate per-round structures with match URLs, team names, and relative file paths for easy navigation.\n",
    "3. Writing the refreshed index.json keeps validation steps and future analytics consistent with the files we have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4132d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote rebuilt index to 24-25_PL_Data/index.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('24-25_PL_Data/index.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild index.json from downloaded match files if index.json is not built correctly. index.json is used to give us a very high level review of each matches file, where it stored, and where we can countercheck if it's information is correct.\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _extract_teams_from_payload_dict(payload: dict):\n",
    "    header = payload.get('header') if isinstance(payload, dict) else None\n",
    "    if isinstance(header, dict):\n",
    "        teams = header.get('teams')\n",
    "        if isinstance(teams, list) and len(teams) >= 2:\n",
    "            home = teams[0].get('name') if isinstance(teams[0], dict) else None\n",
    "            away = teams[1].get('name') if isinstance(teams[1], dict) else None\n",
    "            if home or away:\n",
    "                return home, away\n",
    "        home_name = header.get('home', {}).get('name') if isinstance(header.get('home'), dict) else None\n",
    "        away_name = header.get('away', {}).get('name') if isinstance(header.get('away'), dict) else None\n",
    "        if home_name or away_name:\n",
    "            return home_name, away_name\n",
    "\n",
    "    general = payload.get('general') if isinstance(payload, dict) else None\n",
    "    if isinstance(general, dict):\n",
    "        home = general.get('homeTeam', {}).get('name') if isinstance(general.get('homeTeam'), dict) else None\n",
    "        away = general.get('awayTeam', {}).get('name') if isinstance(general.get('awayTeam'), dict) else None\n",
    "        if home or away:\n",
    "            return home, away\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def rebuild_index_from_files(data_root: str = DATA_ROOT, output_path: Path | None = None) -> Path | None:\n",
    "    data_root_path = Path(data_root)\n",
    "    if not data_root_path.exists():\n",
    "        print(f\"Data root {data_root} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    rounds = []\n",
    "    for round_dir in sorted(data_root_path.glob('round_*')):\n",
    "        if not round_dir.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            round_no = int(round_dir.name.split('_', 1)[1])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "        matches = []\n",
    "        for json_path in sorted(round_dir.glob('*.json')):\n",
    "            stem = json_path.stem\n",
    "            match_id_part = stem.split('_', 1)[0]\n",
    "            if not match_id_part.isdigit():\n",
    "                continue\n",
    "\n",
    "            match_id = int(match_id_part)\n",
    "            home_team = away_team = None\n",
    "            try:\n",
    "                payload = json.loads(json_path.read_text(encoding='utf-8'))\n",
    "                home_team, away_team = _extract_teams_from_payload_dict(payload)\n",
    "            except Exception as exc:\n",
    "                print(f\"Warning: failed to parse {json_path}: {exc}\")\n",
    "\n",
    "            matches.append({\n",
    "                'matchId': match_id,\n",
    "                'matchUrl': f\"https://www.fotmob.com/match/{match_id}\",\n",
    "                'home': home_team,\n",
    "                'away': away_team,\n",
    "                'jsonPath': str(json_path.relative_to(data_root_path)),\n",
    "            })\n",
    "\n",
    "        rounds.append({\n",
    "            'round': round_no,\n",
    "            'matches': matches,\n",
    "        })\n",
    "\n",
    "    rounds.sort(key=lambda item: item['round'])\n",
    "    if output_path is None:\n",
    "        output_path = data_root_path / 'index.json'\n",
    "\n",
    "    output_path.write_text(json.dumps(rounds, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    print(f\"Wrote rebuilt index to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "rebuild_index_from_files()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
