# ğŸ¯ Target Metric Experiments - Overview

## What Your Friend Suggested

> "I would say keep it going with just xG and then let's get the best model from that. Maybe Muhammad Raka Zuhdi could try and find a target metric for liveliness that captures:
> - Shot volume and quality (No. Of shots, xG)
> - End-to-end attacking
> - Lots of chances (so low xG, but many opportunities)
> - Transitions (intense with low quality)
> - Cards
>
> I would lowkey take a subset of the data and then just take the score that we have right now, make multiple versions of the target with each being less complex and see what model performs best."

## What We Built

A systematic experiment framework to test **7 different target metrics** and find the optimal one.

---

## ğŸ“‹ The 7 Target Metrics

### 1ï¸âƒ£ Simple xG (Current Baseline)
```
xG_total + min(xG_home, xG_away)
```
- **Focus:** Pure attacking threat + competitiveness
- **Complexity:** Low
- **Current RÂ²:** 0.088

### 2ï¸âƒ£ Shot Quality
```
0.5 Ã— xG_total + 0.3 Ã— shots_total + 0.2 Ã— sot_total
```
- **Focus:** Quality (xG) + Volume (shots) + Accuracy (SoT)
- **Complexity:** Low-Medium
- **Captures:** Shot volume and quality âœ“

### 3ï¸âƒ£ Chances-Focused
```
0.4 Ã— shots_total + 0.3 Ã— bigch_total + 0.3 Ã— xG_total
```
- **Focus:** Many opportunities (even if low quality)
- **Complexity:** Low-Medium
- **Captures:** Lots of chances âœ“

### 4ï¸âƒ£ End-to-End Attacking
```
xG_total + 2 Ã— min(xG_home, xG_away) + min(shots_home, shots_away)
```
- **Focus:** Competitive matches with both teams attacking
- **Complexity:** Medium
- **Captures:** End-to-end attacking âœ“

### 5ï¸âƒ£ Intensity (Transitions + Cards)
```
0.3 Ã— xG + 0.2 Ã— shots + 0.2 Ã— corners + 0.15 Ã— cards + 0.15 Ã— bigch
```
- **Focus:** Fast-paced, intense matches
- **Complexity:** Medium-High
- **Captures:** Transitions + Cards âœ“

### 6ï¸âƒ£ Comprehensive (All Factors)
```
0.25 Ã— xG + 0.20 Ã— shots + 0.15 Ã— sot + 0.15 Ã— bigch + 0.15 Ã— corners + 0.10 Ã— cards
```
- **Focus:** Everything combined
- **Complexity:** High
- **Captures:** All factors âœ“âœ“âœ“

### 7ï¸âƒ£ Minimal
```
0.6 Ã— xG_total + 0.4 Ã— shots_total
```
- **Focus:** Simplest possible (just xG + shots)
- **Complexity:** Very Low
- **Captures:** Basic shot volume and quality âœ“

---

## ğŸ”¬ Experiment Design

### Data Split (Chronological)
- **Train:** Rounds 0-27 (280 matches)
- **Val:** Rounds 28-32 (50 matches)
- **Test:** Rounds 33-37 (50 matches)

### Features Used
- Same 37 features as current best model
- No data leakage (chronological split)

### Evaluation Metrics
1. **RÂ²** - How much variance explained?
2. **MAE** - Average prediction error
3. **Spearman Ï** - Ranking quality
4. **Top-10 Hit Rate** - Can we identify exciting matches?

### Models Tested
- **Phase 1:** Ridge Regression (for fair comparison)
- **Phase 2:** Ridge, Elastic Net, XGBoost, Gradient Boosting (on best target)

---

## ğŸš€ How to Run

### Option 1: Run Everything at Once (Recommended)
```bash
cd target_metric_experiments
./run_all.sh
```
**Time:** ~5 minutes

### Option 2: Step by Step
```bash
# Step 1: Create targets
python 01_create_alternative_targets.py

# Step 2: Compare targets
python 02_compare_target_metrics.py

# Step 3: Train best target
python 03_train_best_target.py
```

---

## ğŸ“Š What You'll Get

### Immediate Answers:
1. âœ… Which target metric is most predictable?
2. âœ… Does adding cards/corners help?
3. âœ… Should we use a simple or complex formula?
4. âœ… Which model works best for the winning target?

### Files Generated:
```
ğŸ“„ target_metrics_comparison_report.txt  â­ Main findings
ğŸ“ˆ target_metrics_comparison.png         ğŸ“Š Visual comparison
ğŸ“„ best_target_models_report.txt         â­ Best model recommendation
ğŸ“ˆ best_target_models_comparison.png     ğŸ“Š Model comparison
ğŸ“Š targets_comparison.csv                 Raw data
ğŸ“Š target_metrics_comparison_results.csv  Detailed metrics
```

---

## ğŸ¯ Expected Outcomes

### Scenario A: New Target Wins ğŸ‰
- **RÂ² improves to 0.12-0.15** (40% better)
- **Top-10 hit rate â†’ 50-60%**
- **Action:** Update main pipeline to use new target

### Scenario B: Simple xG Still Best ğŸ¤·
- **Current baseline holds**
- **Action:** Focus on getting more data (2022/23, 2023/24 seasons)
- **Insight:** Target metric isn't the bottleneck

### Scenario C: Close Call ğŸ¤”
- **Multiple targets perform similarly**
- **Action:** Choose based on interpretability or use case
- **Option:** Ensemble different targets

---

## ğŸ’¡ Key Insights You'll Gain

1. **Complexity vs Performance Trade-off**
   - Does a complex formula actually help?
   - Or is simple xG already optimal?

2. **Feature Importance**
   - Which match characteristics matter most?
   - Cards? Corners? Shot volume?

3. **Model Selection**
   - Is Ridge still best?
   - Or does XGBoost/Gradient Boosting work better with new target?

4. **Practical Utility**
   - Can we better identify exciting matches?
   - Does Top-10 hit rate improve?

---

## ğŸ“ Notes

- All experiments use **same features** (fair comparison)
- **No data leakage** (chronological splits)
- **Reproducible** (fixed random seeds)
- **Fast** (~5 minutes total)

---

## ğŸ“ For Your Friend

This addresses all their suggestions:
- âœ… Multiple target versions (7 variants)
- âœ… Varying complexity (simple â†’ comprehensive)
- âœ… Captures all desired factors (shots, xG, chances, transitions, cards)
- âœ… Uses subset approach (train/val/test splits)
- âœ… Tests best model for each target
- âœ… Data-driven recommendation

---

## ğŸš¦ Ready to Start?

```bash
cd target_metric_experiments
./run_all.sh
```

Then read the two report files for your answer! ğŸ“„â­
